{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdee4f50-ab59-4f74-bfac-65939bd69054",
   "metadata": {},
   "source": [
    "# Breast Cancer - Diagnostic (Wisconsin)\n",
    "https://juliaai.github.io/DataScienceTutorials.jl/end-to-end/breastcancer/#breast_cancer_wisconsindiagnostic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a88258f-a1c2-4b3e-af99-ca94d80185c5",
   "metadata": {},
   "source": [
    "## Import and Load Relevant Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d8b72d-493b-4295-a941-42faf19617ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg; Pkg.add([\"UrlDownload\",\"DataFrames\",\"PrettyPrinting\",\"PyPlot\",\"MLJ\",\"ScientificTypesBase\",\"MLJScikitLearnInterface\",\"ScikitLearn\",\"PyCall\",\"CSV\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee241225-c87a-4193-8cbe-5dbbdbf9c9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(`pip3 install sklearn`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073d780b-e551-4592-80ba-f1a07318016e",
   "metadata": {},
   "outputs": [],
   "source": [
    "using UrlDownload\n",
    "using DataFrames\n",
    "using PrettyPrinting\n",
    "using PyPlot\n",
    "using MLJ\n",
    "using MLJScikitLearnInterface\n",
    "using ScikitLearn\n",
    "using PyCall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3da471-a0dc-4fd6-aec6-4b701b6d2746",
   "metadata": {},
   "source": [
    "Inititalizing a global random seed which we'll use throughout the code to maintain consistency in results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf4c43c-323e-4bc3-b984-40e360a6c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed76cf3-7bb3-4d00-a3cb-136d8502e29a",
   "metadata": {},
   "source": [
    "## Download the Data\n",
    "Using the package **UrlDownload.jl**, we can capture the data from the given link using the below commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d9ba46-891e-45c7-8704-aa869c44a8ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data\";\n",
    "feature_names = [\"ID\", \"Class\", \"mean radius\", \"mean texture\", \"mean perimeter\", \"mean area\", \"mean smoothness\", \"mean compactness\", \"mean concavity\", \"mean concave points\", \"mean symmetry\", \"mean fractal dimension\", \"radius error\", \"texture error\", \"perimeter error\", \"area error\", \"smoothness error\", \"compactness error\", \"concavity error\", \"concave points error\", \"symmetry error\", \"fractal dimension error\", \"worst radius\", \"worst texture\", \"worst perimeter\", \"worst area\", \"worst smoothness\", \"worst compactness\", \"worst concavity\", \"worst concave points\", \"worst symmetry\", \"worst fractal dimension\"]\n",
    "data = urldownload(url, true, format = :CSV, header = feature_names);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1259687-7c0d-4178-9623-cf5ed0f9540e",
   "metadata": {},
   "source": [
    "## Explore downloaded data\n",
    "### Inspecting the class variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29b3942-1e57-4fa7-a7cf-432fe7f65a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(8, 6))\n",
    "hist(data.Class)\n",
    "xlabel(\"Classes\")\n",
    "ylabel(\"Number of samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367da8c0-8977-454e-8e8f-0dba7dc49c36",
   "metadata": {},
   "source": [
    "### Inspect the feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428d115f-b9d4-400a-b2c8-eacf9407f717",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataFrame(data)[:, 2:end];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b7f8f3-feee-4a7a-9c4f-3b8477c5de89",
   "metadata": {},
   "source": [
    "Printing the 1st 10 rows so as to get a visual idea about the type of data we're dealing with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd23366-ea01-49e6-a667-116323a11a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(first(df,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee4d403-b1bb-479b-b550-3069dc883b5e",
   "metadata": {},
   "source": [
    "For checking the statistical attributes of each inividual feature, we can use the decsribe() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea14f0cd-00d3-4cef-8187-70aedbb5d12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(describe(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8256054e-07fa-4b4d-8a27-a63bfae7d658",
   "metadata": {},
   "source": [
    "As we can see the feature set consists of varying features that have different ranges and quantiles. This can cause trouble for the optimization techniques and might cause convergence issues. We can use a feature scaling technique like **Standardizer()** to handle this.\n",
    "\n",
    "But first, let's handle the scientific types of all the features. We can use the **schema()** method from **MLJ.jl** package to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cbd834-561f-4a9c-8315-bf111410e828",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(schema(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3beb8470-3bd2-4293-acb4-d039cdcb5091",
   "metadata": {},
   "source": [
    "As the target variable is 'Textual' in nature, we'll have to change it to a more appropriate scientific type. Using the **coerce()** method, let's change it to an **OrderedFactor**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bbae9a1-83fc-44f5-bbfd-9e5f26ee959e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coerce!(df, :Class => OrderedFactor{2});"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3ca412-4724-414d-ad1f-3d72bda772f8",
   "metadata": {},
   "source": [
    "## Unpacking the Values\n",
    "\n",
    "Now that our data is fully processed, we can separate the target variable 'y' from the feature set 'X' using the unpack() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3233ac2-322b-4ce2-8a8f-057e2795ab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = unpack(df, ==(:Class),name->true, rng = RANDOM_SEED);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f2c907-0f97-4e3c-97d2-ea7a60c5c08f",
   "metadata": {},
   "source": [
    "## Standardizing the \"feature set\"\n",
    "Now that our feature set is separated from the target variable, we can use the Standardizer() worklow to obtain to standadrize our feature set 'X'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0814e99-4cd4-41c0-993d-f7615624820e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_instance = Standardizer()\n",
    "transformer_model = machine(transformer_instance, X)\n",
    "MLJ.fit!(transformer_model)\n",
    "X = MLJ.transform(transformer_model, X);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c19a68-41e7-427a-9bde-cedd0decb71b",
   "metadata": {},
   "source": [
    "## Train-test split\n",
    "After feature scaling, our data is ready to put into a Machine Learning model for classification! Using 80% of data for training, we can perform a train-test split using the partition() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99697a9a-4ec9-4612-b20c-7b1b5bcadaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = partition(eachindex(y), 0.8, shuffle=true, rng=RANDOM_SEED);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69911f82-c8b0-4817-8462-1716ced61cd0",
   "metadata": {},
   "source": [
    "## Model compatibility\n",
    "Now that we have separate training and testing set, let's see the models compatible with our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2b3e7e-5091-4f70-b984-a8a60f028a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in models(matching(X, y))\n",
    "    println(\"Model name = \",m.name,\", \",\"Prediction type = \",m.prediction_type,\", \",\"Package name = \",m.package_name);\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a074dad-cd0f-4b14-a236-99b713eba61c",
   "metadata": {},
   "source": [
    "## Analyzing the performance of different models\n",
    "Thats a lot of models for our data! To narrow it down, lets analyze the performance of \"probabilistic classifiers\" from the \"ScikitLearn\" package.\n",
    "### Creating various empty vectors for our analysis\n",
    "* model_names captures the names of the models being iterated\n",
    "* loss_acc captures the value of the model accuracy on the test set\n",
    "* loss_ce captures the values of the Cross-entropy loss on the test set\n",
    "* loss_f1 captures the values of F1-Score on the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0ef8806-67b3-4f40-9103-aef982166cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names=Vector{String}();\n",
    "loss_acc=[];\n",
    "loss_ce=[];\n",
    "loss_f1=[];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92ed44f4-2563-4b89-afed-0d674e1d6340",
   "metadata": {},
   "source": [
    "### Collecting data for analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ae57d4-90fc-43ef-88f1-9cedee883858",
   "metadata": {},
   "source": [
    "Install sklearn via pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2321ee-6dbc-497b-acc8-9902491b48be",
   "metadata": {},
   "outputs": [],
   "source": [
    "run(`pip3 list`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955277e2-1783-458e-934d-2b266de6f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "figure(figsize=(8, 6))\n",
    "for m in models(matching(X, y))\n",
    "    if m.prediction_type==Symbol(\"probabilistic\") && m.package_name==\"ScikitLearn\" && m.name!=\"LogisticCVClassifier\"\n",
    "        #Excluding LogisticCVClassfiier as we can infer similar baseline results from the LogisticClassifier\n",
    "\n",
    "        #Capturing the model and loading it using the @load utility\n",
    "        model_name=m.name\n",
    "        package_name=m.package_name\n",
    "        eval(:(clf = @load $model_name pkg=$package_name verbosity=0))\n",
    "\n",
    "        #Fitting the captured model onto the training set\n",
    "        clf_machine = machine(clf(), X, y)\n",
    "        fit!(clf_machine, rows=train)\n",
    "\n",
    "        #Getting the predictions onto the test set\n",
    "        y_pred = MLJ.predict(clf_machine, rows=test);\n",
    "\n",
    "        #Plotting the ROC-AUC curve for each model being iterated\n",
    "        fprs, tprs, thresholds = roc(y_pred, y[test])\n",
    "        plot(fprs, tprs,label=model_name);\n",
    "\n",
    "        #Obtaining different evaluation metrics\n",
    "        ce_loss = mean(cross_entropy(y_pred,y[test]))\n",
    "        acc = accuracy(mode.(y_pred), y[test])\n",
    "        f1_score = f1score(mode.(y_pred), y[test])\n",
    "\n",
    "        #Adding the different obtained values of the evaluation metrics to the respective vectors\n",
    "        push!(model_names, m.name)\n",
    "        append!(loss_acc, acc)\n",
    "        append!(loss_ce, ce_loss)\n",
    "        append!(loss_f1, f1_score)\n",
    "    end\n",
    "end\n",
    "\n",
    "#Adding labels and legend to the ROC-AUC curve\n",
    "xlabel(\"False Positive Rate\")\n",
    "ylabel(\"True Positive Rate\")\n",
    "legend(loc=\"best\", fontsize=\"xx-small\")\n",
    "title(\"ROC curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b166d9-71c0-4420-bfe5-63b043ce305b",
   "metadata": {},
   "source": [
    "## Analyzing models\n",
    "Let's collect the data in form a dataframe for a more precise analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6f7e8b-bb5b-4b47-9cd5-3c76d20cc19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_info=DataFrame(ModelName=model_names,Accuracy=loss_acc,CrossEntropyLoss=loss_ce,F1Score=loss_f1);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7882573-c8e3-4bc4-b138-61861d40a829",
   "metadata": {},
   "source": [
    "Now, let's sort the data on basis of the Cross-entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b1f15-869b-4811-b122-8eef0e261c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(sort!(model_info,[:CrossEntropyLoss]));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac43ba8-48bc-4123-87e0-63ba2e455b31",
   "metadata": {},
   "source": [
    "It seems like a simple LogisticClassifier works really well with this dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4114f5-184a-4561-a660-60b15ce22849",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "This article covered iterative feature selection on the Breast cancer classification dataset. In this tutorial, we only analyzed the ScikitLearn models so as to keep the flow of the content precise, but the same workflow can be applied to any compatible model in the MLJ family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed485f4a-5fc1-4cc4-8713-c0acc77e8c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
